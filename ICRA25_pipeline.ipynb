{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for \"Bootstrapping Object-level Planning with Large Language Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 :- Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries for LLM prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olp_lib import *\n",
    "\n",
    "from coppeliasim.generate import randomize_blocks, all_letters\n",
    "from OMPLement.utils import *\n",
    "from pddl_planning import *\n",
    "\n",
    "from random import choice, sample, randint\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import traceback\n",
    "\n",
    "# NOTE: keep checking the pricing for details on best model to use: https://openai.com/api/pricing/\n",
    "\n",
    "models = [\"chatgpt-4o-latest\", \"gpt-4o\",]\n",
    "\n",
    "# -- use the custom-made class for accessing OpenAI models:\n",
    "openai_driver = OpenAIInterfacer(\n",
    "\tmodel_embed=\"text-embedding-3-small\",\n",
    "    model_text_gen=models[-1],\n",
    ")\n",
    "\n",
    "# NOTE: all incontext examples will be stored within a JSON file:\n",
    "# -- Q: can we randomly sample from the set of incontext examples?\n",
    "# -- Q: should we also select an example \"closest\" to the provided task?\n",
    "\n",
    "incontext_file = \"all_fewshot_examples.json\"\n",
    "fewshot_examples = json.load(open(incontext_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation and Task Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: simulation settings (do not change):\n",
    "coppelia_robot = \"Panda\"\n",
    "coppelia_gripper = \"Panda_gripper\"\n",
    "coppelia_port_number = None\n",
    "\n",
    "# NOTE: experiment settings (do not change):\n",
    "setting = \"blocks\"\n",
    "is_alphabetic = True\n",
    "terminate_upon_failure = True\n",
    "experimental_results = []\n",
    "\n",
    "verbose = True\n",
    "run_simulation = True\n",
    "\n",
    "# -- create a folder to store the results from a certain run:\n",
    "timestamp = dt.today().strftime('%Y-%m-%d_%H-%M-%SS')\n",
    "results_dir = f\"results_{timestamp}/\"\n",
    "if not os.path.exists(os.path.join(os.getcwd(), results_dir)):\n",
    "    os.mkdir(os.path.join(os.getcwd(), results_dir))\n",
    "\n",
    "# NOTE: distribution of blocks for organizing task:\n",
    "options_organize = [\n",
    "    [2, 2, 2],\n",
    "    [3, 2, 2],\n",
    "    [3, 3, 2],\n",
    "    [3, 3, 3],\n",
    "    [4, 3, 3],\n",
    "    [4, 4, 3],\n",
    "    [4, 4, 4],\n",
    "    [4, 4, 4, 4],\n",
    "]\n",
    "\n",
    "# NOTE: words for spelling task:\n",
    "options_spelling = [\n",
    "    list(\"LOL\"),\n",
    "    list(\"CAT\"),\n",
    "    list(\"FUN\"),\n",
    "    list(\"PLAN\"),\n",
    "    list(\"TREE\"),\n",
    "    list(\"GIFT\"),\n",
    "    list(\"SKILL\"),\n",
    "    list(\"THIEF\"),\n",
    "    list(\"JESTER\"),\n",
    "    list(\"MASTER\"),\n",
    "    list(\"NOTION\"),\n",
    "    list(\"DREAMER\"),\n",
    "    list(\"FREEDOM\"),\n",
    "    list(\"TACTICS\"),\n",
    "    list(\"UPPERCUT\"),\n",
    "    list(\"HI_THERE\"),\n",
    "]\n",
    "\n",
    "# NOTE: for the case of the \"spelling\" task, include distractor objects\n",
    "use_distractors = True \n",
    "\n",
    "# NOTE: heights for \"towers\" task:\n",
    "options_towers = [x for x in range(3, 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Task Setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only select one of the following options:\n",
    "task = choice([\"towers\", \"spelling\", \"organize\"])\n",
    "# task = \"towers\"\n",
    "# task = \"organize\"\n",
    "# task = \"spelling\"\n",
    "\n",
    "task_setting = choice(eval(f\"options_{task}\"))\n",
    "\n",
    "print(task, task_setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Attributes Dict for Random Scene Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_simulation:\n",
    "    # -- select a random subset of letters:\n",
    "    attributes = {\"alphabets\": {}}\n",
    "\n",
    "    if task == \"spelling\":\n",
    "        # -- add some distractor blocks in the scene:\n",
    "        all_blocks = list(task_setting)\n",
    "\n",
    "        if use_distractors:\n",
    "            all_blocks += list(sample(all_letters, randint(1, len(task_setting))))\n",
    "\n",
    "        for x in range(len(all_blocks)):\n",
    "            # -- get each character forming the word we want to spell:\n",
    "            character = all_blocks[x]\n",
    "\n",
    "            # -- we can also handle spaces:\n",
    "            if character in [' ', '_']: character = 'whitespace'\n",
    "\n",
    "            if character not in attributes['alphabets']:\n",
    "                attributes['alphabets'][character] = {'count': 0}\n",
    "\n",
    "            attributes['alphabets'][character]['count'] += 1\n",
    "\n",
    "\n",
    "    elif task == \"towers\":\n",
    "        subset = sample(all_letters, task_setting+1)\n",
    "        for x in range(len(subset)):\n",
    "            attributes['alphabets'].update({subset[x]: {\"count\": 1}})\n",
    "\n",
    "    elif task == \"organize\":\n",
    "        subset = sample(all_letters, len(task_setting))\n",
    "        for x in range(len(subset)):\n",
    "            attributes['alphabets'].update({subset[x]: {\"count\": task_setting[x]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start CoppeliaSim:\n",
    "**NOTE**: Uncomment this block with the path to your CoppeliaSim installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_simulation:\n",
    "#     if os.name == 'nt':\n",
    "#         cmd_line = [\"C:/Program Files/CoppeliaRobotics/CoppeliaSimEdu/coppeliaSim.exe\", f\"-GzmqRemoteApi.rpcPort={coppelia_port_number}\"]\n",
    "#     else:\n",
    "#         cmd_line = [\"../CoppeliaSim/coppeliaSim\", f\"-GzmqRemoteApi.rpcPort={coppelia_port_number}\"]\n",
    "\n",
    "#     coppelia_process = subprocess.Popen(\n",
    "#         cmd_line,\n",
    "#         stdout=subprocess.DEVNULL,\n",
    "#         stderr=subprocess.STDOUT\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize simulation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- we provide a prototype scene with which we generate a random scene for the task:\n",
    "fpath = f\"./coppeliasim/scenes/panda_{setting}_prototype.ttt\"\n",
    "sim_interfacer = Interfacer(\n",
    "    scene_file_name=fpath,\n",
    "    robot_name=coppelia_robot,\n",
    "    robot_gripper=coppelia_gripper,\n",
    "    port_number=coppelia_port_number,\n",
    ")\n",
    "\n",
    "# -- we have different functions depending on the scene type:\n",
    "block_type = 'alphabets' if is_alphabetic else 'colours'\n",
    "fpath, tally = randomize_blocks(\n",
    "    sim_interfacer,\n",
    "    path=os.path.join(results_dir, os.path.basename(fpath)),\n",
    "    attributes=attributes,\n",
    "    block_type=block_type,\n",
    "    suffix=timestamp,\n",
    ")\n",
    "\n",
    "print(os.path.join(results_dir, os.path.basename(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'*' * 25}\\nSCENE HAS BEEN GENERATED\\n{'*' * 25}\")\n",
    "\n",
    "object_phrases = []\n",
    "for item in tally:\n",
    "    for C in tally[item]:\n",
    "        if bool(tally[item][C]):\n",
    "            num_as_text = \"one\" if tally[item][C] == 1 else \"two\" if tally[item][C] == 2 else \"three\" if tally[item][C] == 3 else \"four\" if tally[item][C] == 4 else \"five\"\n",
    "            object_phrases.append(\n",
    "                    f\"{num_as_text} ({tally[item][C]}) \"\n",
    "                    # + str(('' if not bool(is_alphabetic) else '\\'') + f\"{C}\" + ('' if not bool(is_alphabetic) else '\\''))\n",
    "                    f\"{C if C not in [' ', '_'] else 'white-space'}{(' ' if not bool(is_alphabetic) or C in [' ', '_'] else '-')}{item}{'s' if tally[item][C] > 1 else ''}\"\n",
    "            )\n",
    "\n",
    "sim_interfacer = Interfacer(\n",
    "    scene_file_name=fpath,\n",
    "    robot_name=coppelia_robot,\n",
    "    robot_gripper=coppelia_gripper,\n",
    "    port_number=coppelia_port_number,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'*' * 4} AFTER COPPELIASIM RANDOMIZATION: {'*' * 4}\")\n",
    "print(\"objects presently in scene:\", sim_interfacer.objects_in_sim)\n",
    "print(\"\\nobjects in sim for LLM:\", object_phrases)\n",
    "\n",
    "user_task = {\n",
    "    \"objects\": object_phrases,\n",
    "    \"query\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Planning/Execution Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these are the original names of the OLP-based method and baselines:\n",
    "all_methods = [\n",
    "    'OLP', 'OLP-UTAMP',\n",
    "\t'FOON-UTAMP', 'FOON',\n",
    "    'LLM-Planner', 'LLM-Planner-UTAMP',\n",
    "    'LLM-UTAMP',\n",
    "    'LLM+P'\n",
    "]\n",
    "\n",
    "# evaluated_methods = ['FOON', 'LLM-Planner', 'LLM+P', 'DELTA']\n",
    "evaluated_methods = [\n",
    "\t'FOON',\n",
    "\t'LLM-Planner',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt User for Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = None\n",
    "if task == \"spelling\":\n",
    "    arg = str(\"\".join(task_setting))\n",
    "elif task == \"towers\":\n",
    "    arg = int(task_setting)\n",
    "\n",
    "user_task['query'] = None\n",
    "task_file_name = None\n",
    "# task = None\n",
    "\n",
    "if user_task['query'] is None:\n",
    "\n",
    "    if task == \"organize\":\n",
    "        user_task['query'] = choice([\n",
    "            \"Organize the table by stacking all similar blocks into piles.\",\n",
    "            \"Make piles of matching blocks on the table.\"\n",
    "        ])\n",
    "        task_file_name = f\"organize_table.txt\"\n",
    "\n",
    "    elif task == \"spelling\":\n",
    "        # -- maybe we can spell one word or two words:\n",
    "        words = [arg]\n",
    "        if len(words) == 1:\n",
    "            user_task['query'] = choice([\n",
    "                f\"Spell \\\"{words[0]}\\\" as a block tower.\",\n",
    "                f\"Make a tower of blocks spelling out \\\"{words[0]}\\\".\",\n",
    "                # f\"Spell the word \\\"{words[0]}\\\" in reverse.\",\n",
    "                # f\"Make a tower of blocks spelling out the word \\\"{words[0]}\\\" in reverse.\",\n",
    "            ])\n",
    "            task_file_name = f\"spell_the_word_{words[0]}.txt\"\n",
    "        else:\n",
    "            user_task['query'] = choice([\n",
    "                f\"Spell \\\"{words[0]}\\\" and \\\"{words[1]}\\\" vertically.\",\n",
    "                f\"Make 2 towers that spell \\\"{words[1]}\\\" and \\\"{words[0]}\\\".\",\n",
    "                f\"Spell \\\"{words[0]}\\\" and \\\"{words[1]}\\\" in reverse.\",\n",
    "                f\"Make 2 towers of blocks spelling \\\"{words[0]}\\\" and \\\"{words[1]}\\\" but in reverse.\",\n",
    "            ])\n",
    "            task_file_name = f\"spell_the_words_{words[0]}_and_{words[1]}.txt\"\n",
    "\n",
    "        user_task['query'] += (\n",
    "            \" Spell the word vertically to read from top to bottom.\"\n",
    "            \" For example, to spell \\\"BOY\\\", you must stack each letter in reverse: first place the letter \\\"O\\\" on top of the letter \\\"Y\\\", then the letter \\\"B\\\" on top of the letter \\\"O\\\" so it reads \\\"BOY\\\" from top to bottom.\"\n",
    "        )\n",
    "    elif task == \"towers\":\n",
    "        user_task['query'] = choice([\n",
    "                f\"Stack exactly {arg} blocks.\",\n",
    "                f\"Make a tower with {arg} blocks.\",\n",
    "            ])\n",
    "        task_file_name = f\"stack_{arg}_blocks.txt\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        user_task['objects'] = [\n",
    "            \"cup of lemon juice\", \"lemon juice\",\n",
    "            \"cup of ice\", \"ice\",\n",
    "            \"bottle of club soda\", \"club soda\",\n",
    "            \"bottle of vodka\", \"vodka\",\n",
    "            \"bottle of tonic water\", \"tonic water\",\n",
    "            \"bottle of Worcestershire sauce\", \"Worcestershire sauce\",\n",
    "            \"bottle of cola\", \"cola\",\n",
    "            \"bottle of water\", \"water\",\n",
    "            \"bottle of orange juice\", \"orange juice\",\n",
    "            \"can of tomato juice\", \"tomato juice\",\n",
    "            \"shaker of salt\", \"salt\",\n",
    "            \"shaker of black pepper\", \"black pepper\",\n",
    "            \"drinking glass\",\n",
    "            \"spoon\",\n",
    "            \"celery\",\n",
    "        ]\n",
    "        while not user_task['query']:\n",
    "            user_task['query'] = input(f\"The robot has the following objects available to it on a table: {str(user_task['objects'])}.\\nWhat would you like the robot to do?\")\n",
    "\n",
    "        task_file_name = f\"{str.lower(re.compile('[^a-zA-Z]').sub('_', user_task['query'])[:-1])}.txt\"\n",
    "\n",
    "print('User Task:', user_task['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 :- LLM -> Object-level Plan\n",
    "\n",
    "1. Initialize libraries needed for FOON (``FOON_graph_analyser.py``) as well as OpenAI api.\n",
    "\n",
    "2. Perform 2-stage prompting for recipe prototype.\n",
    "    - In the first stage, we ask the LLM for a *high-level recipe* (list of instructions) and a *list of objects* needed for completing the recipe.\n",
    "    - In the second stage, we ask the LLM for a breakdown of *state changes* that happen for each step of the recipe; specifically, we ask for the *preconditions* and *effects* of each action, which is similar to how a functional unit in FOON has *input* and *output object nodes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOON-based OLP Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['FOON', 'FOON-UTAMP']) & set(evaluated_methods)):\n",
    "\n",
    "    print(f\"\\n\\nUser Task: {user_task['query']}\")\n",
    "    print(f\"Available objects: {user_task['objects']}\")\n",
    "\n",
    "    # -- load prototype/reference FOON graphs:\n",
    "    loaded_foons = []\n",
    "    for foon_file in os.listdir('./foon_prototypes/'):\n",
    "        fga._resetFOON(); fga._constructFOON(os.path.join('./foon_prototypes/', foon_file))\n",
    "        loaded_foons.append({\n",
    "            \"foon\": fga.FOON_lvl3,\n",
    "        })\n",
    "        print(codify_FOON(loaded_foons[-1]['foon']))\n",
    "\n",
    "    total_time = time.time()\n",
    "\n",
    "    llm_output, interaction = generate_from_FOON(\n",
    "        openai_obj=openai_driver,\n",
    "        query=user_task['query'],\n",
    "        FOON_samples=loaded_foons,\n",
    "        scenario={\n",
    "            \"name\": setting,\n",
    "            \"objects\": user_task['objects'] if isinstance(user_task, dict) else None,\n",
    "        },\n",
    "        system_prompt_file='llm_prompts/foon_system_prompt.txt',\n",
    "        human_feedback=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # -- save unparsed output to a JSON file:\n",
    "    json.dump(llm_output, open('raw_FOON.json', 'w'), indent=4)\n",
    "\n",
    "    print(\"\\nTASK:\",  user_task)\n",
    "    print(f\"\\n{'*' * 25}\\n    High level plan\\n{'*' * 23}\\n{llm_output['language_plan']}\")\n",
    "    print(f\"\\nAll Objects: {llm_output['all_objects']}\")\n",
    "    print(f\"\\n\\n{'*' * 25}\\n    DETAILED \\n{'*' * 23}\\n{json.dumps(llm_output['object_level_plan'], indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular OLP Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['OLP', 'OLP-UTAMP']) & set(evaluated_methods)):\n",
    "    # -- using the \"legacy\" OLP prompting method:\n",
    "    print('User Task:', user_task['query'])\n",
    "    print('Available objects:', user_task['objects'])\n",
    "\n",
    "    llm_output = None\n",
    "\n",
    "    use_cache = False\n",
    "\n",
    "    if use_cache:\n",
    "        while True:\n",
    "            llm_output = repair_olp(\n",
    "                openai_obj=openai_driver,\n",
    "                query=user_task['query'],\n",
    "                available_objects=user_task['objects'],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            if isinstance(llm_output, int):\n",
    "                if llm_output == 0:\n",
    "                    # -- return 0 means that there are no items in cache:\n",
    "                    llm_output = None\n",
    "                    break\n",
    "                elif llm_output == -1:\n",
    "                    # -- return -1 means that some error was encountered (e.g. LLM did not say \"okay\"):\n",
    "                    pass\n",
    "\n",
    "            else: break\n",
    "\n",
    "    if not llm_output:\n",
    "        # -- stick to the regular OLP creation pipeline powered by LLM goodness:\n",
    "        while True:\n",
    "            try:\n",
    "                llm_output, interaction = generate_olp(\n",
    "                    openai_driver,\n",
    "                    query=user_task[\"query\"],\n",
    "                    fewshot_examples=fewshot_examples,\n",
    "                    scenario={\n",
    "                        \"name\": setting,\n",
    "                        \"objects\": user_task['objects'] if isinstance(user_task, dict) else None,\n",
    "                    },\n",
    "                    stage1_sys_prompt_file=\"llm_prompts/olp_stage1_prompt.txt\",\n",
    "                    stage2_sys_prompt_file=\"llm_prompts/olp_stage2_prompt.txt\",\n",
    "                    stage3_sys_prompt_file=\"llm_prompts/olp_stage3_prompt.txt\",\n",
    "                    stage4_sys_prompt_file=\"llm_prompts/olp_stage4_prompt.txt\",\n",
    "                    verbose=False\n",
    "                )\n",
    "            except Exception as e: print('-- something went wrong:', type(e), e.args)\n",
    "            else:\n",
    "                if llm_output: break\n",
    "\n",
    "    # -- save unparsed output to a JSON file:\n",
    "    json.dump(llm_output, open('raw_OLP.json', 'w'), indent=4)\n",
    "\n",
    "    print(\"\\nTASK:\",  user_task)\n",
    "    print(f\"\\n{'*' * 25}\\n    High level plan\\n{'*' * 23}\\n{llm_output['language_plan']}\\n\")\n",
    "    print(f\"\\All Objects: {llm_output['all_objects']}\")\n",
    "    print(f\"\\n\\n{'*' * 25}\\n    DETAILED \\n{'*' * 23}\\n{json.dumps(llm_output['object_level_plan'], indent=4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 :- Object-level Plan -> FOON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new functional units\n",
    "- This involves parsing a JSON structure produced by the LLM to create FOON functional units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['FOON', 'FOON-UTAMP', 'OLP', 'OLP-UTAMP']) & set(evaluated_methods)):\n",
    "\n",
    "    FOON_prototype = []\n",
    "    FOON_steps = []\n",
    "\n",
    "    for x in range(len(llm_output['object_level_plan']['plan'])):\n",
    "\n",
    "        new_unit = olp_to_FOON(llm_output, index=x)\n",
    "\n",
    "        # NOTE: in order to define a macro-problem, we need to properly identify all goal nodes;\n",
    "        #       we will do this with the help of the LLM:\n",
    "        if 'termination_steps' in llm_output and (x+1) in llm_output['termination_steps']:\n",
    "            # -- set output objects as goal nodes for the functional units deemed as terminal steps:\n",
    "            print(f'\\nFunctional unit {x+1} has terminal goals!')\n",
    "            for N in range(new_unit.getNumberOfOutputs()):\n",
    "                new_unit.getOutputNodes()[N].setAsGoal()\n",
    "\n",
    "        # -- add the functional unit to the FOON prototype:\n",
    "\n",
    "        if not new_unit.isEmpty():\n",
    "            # -- we should only add a new functional unit if it is not empty, meaning it must have the following:\n",
    "            #    1. >=1 input node and >= 1 output node\n",
    "            #    2. a valid motion node\n",
    "            FOON_prototype.append(new_unit)\n",
    "            FOON_prototype[-1].print_functions[-1]()\n",
    "\n",
    "            # -- add the language plan step to a temp list for FOON comments:\n",
    "            FOON_steps.append(llm_output['object_level_plan']['plan'][x]['instruction'])\n",
    "        else:\n",
    "            print('NOTE: the following functional unit has an error, so skipping it:')\n",
    "            new_unit.print_functions[-1]()\n",
    "\n",
    "        print('\\n', '*' * 40)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Generated FOON\n",
    "1. Use ```FOON_parser.py``` to clean and ensure FOON labels are correct\n",
    "2. Add the cleaned FOON to the embedded cache of object-level plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['FOON', 'FOON-UTAMP', 'OLP', 'OLP-UTAMP']) & set(evaluated_methods)):\n",
    "\n",
    "    temp_file_name = 'prototype.txt'\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    preprocess_dir = os.path.join(os.getcwd(), results_dir, 'preprocess/')\n",
    "    postprocess_dir = os.path.join(os.getcwd(), results_dir, 'postprocess/')\n",
    "\n",
    "    # -- save the prototype FOON graph as a text file, which we will then run with a parser to correct numbering:\n",
    "    if not os.path.exists(preprocess_dir):\n",
    "        os.makedirs(preprocess_dir)\n",
    "\n",
    "    if not os.path.exists(postprocess_dir):\n",
    "        os.makedirs(postprocess_dir)\n",
    "\n",
    "    with open(os.path.join(preprocess_dir, temp_file_name), 'w') as prototype:\n",
    "        prototype.write(\"//\\tFOON Prototype\\n\")\n",
    "        prototype.write(f\"//\\t-- Task Prompt: {user_task['query']}\\n\")\n",
    "        prototype.write(f\"//\\t-- Required Objects: {llm_output['all_objects']}\\n\")\n",
    "        prototype.write(\"//\\n\")\n",
    "        for x in range(len(FOON_prototype)):\n",
    "            prototype.write(f\"//Action {x+1}: {FOON_steps[x]}\\n\")\n",
    "            prototype.write(FOON_prototype[x].getFunctionalUnitAsText())\n",
    "            if verbose:\n",
    "                print(f'{FOON_prototype[x].print_functions[2]()}//')\n",
    "\n",
    "    # -- running parsing module to ensure that FOON labels and IDs are made consistent for further use:\n",
    "    #\t\t(it is important that each object and state type have a *UNIQUE* identifier)\n",
    "    fpa.skip_JSON_conversion = True\t\t# -- we don't need JSON versions of a FOON\n",
    "    fpa.skip_index_check = True\t\t\t# -- always create a new set of index files\n",
    "\n",
    "    fpa.source_dir = preprocess_dir\n",
    "    fpa.target_dir = postprocess_dir\n",
    "    fpa._run_parser()\n",
    "\n",
    "    with open(os.path.join(postprocess_dir, temp_file_name), 'r') as input_file:\n",
    "        with open(os.path.join(postprocess_dir, task_file_name), 'w') as output_file:\n",
    "            for line in input_file:\n",
    "                output_file.write(line)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n-- [LLM-to-OLP] : File has been saved as \\\"./postprocess/{task_file_name}\\\"\")\n",
    "        print()\n",
    "        print(os.path.join(os.getcwd(), './postprocess/', task_file_name))\n",
    "\n",
    "    fga._resetFOON()\n",
    "    fga._constructFOON(os.path.join(postprocess_dir, task_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 :- FOON -> ```FOON-TAMP```/```FOON_to_PDDL```\n",
    "1. Parse FOON file -- this step is important to ensure that all labels are unique and that the generated file follows the FOON syntax.\n",
    "\n",
    "2. Run ``FOON_to_PDDL.py`` script to generate FOON macro-operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate PDDL files using ```FOON_to_PDDL``` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_macro_POs = None\n",
    "\n",
    "robot_domain_file = './robot_skills.pddl'\n",
    "\n",
    "use_macro_planning = False\n",
    "\n",
    "# ### Generate PDDL files using ```FOON_to_PDDL``` module\n",
    "\n",
    "if bool(set(['FOON', 'FOON-UTAMP', 'OLP', 'OLP-UTAMP']) & set(evaluated_methods)):\n",
    "\n",
    "    foon_dir = os.path.join(os.getcwd(), f'results_{timestamp}/', 'output_foon')\n",
    "    if not os.path.exists(foon_dir):\n",
    "        os.mkdir(foon_dir)\n",
    "\n",
    "    FOON_subgraph_file = os.path.join(postprocess_dir, task_file_name)\n",
    "    micro_problems_path = f'{foon_dir}/micro_problems-' + Path(FOON_subgraph_file).stem\n",
    "\n",
    "    # -- definition of macro and micro plan file names:\n",
    "    macro_plan_file = os.path.splitext(FOON_subgraph_file)[0] + '_macro.plan'\n",
    "    micro_plan_file = os.path.splitext(FOON_subgraph_file)[0] + '_micro.plan'\n",
    "\n",
    "    import shutil\n",
    "    if os.path.exists(micro_problems_path):\n",
    "        shutil.rmtree(micro_problems_path)\n",
    "\n",
    "    from foon_to_pddl import FOON_to_PDDL as ftp\n",
    "\n",
    "    # -- create a new folder for the generated problem files and their corresponding plans:\n",
    "    os.makedirs(micro_problems_path)\n",
    "\n",
    "    # -- save unparsed output to a JSON file:\n",
    "    json.dump(llm_output, open(f'{micro_problems_path}/{Path(task_file_name).stem}.json', 'w'), indent=4)\n",
    "\n",
    "    # -- perform conversion of the FOON subgraph file to PDDL:\n",
    "    ftp.FOON_subgraph_file = FOON_subgraph_file\n",
    "    ftp._convert_to_PDDL('OCP')\n",
    "\n",
    "    ## -- parse through the newly created domain file and find all (macro) planning operators:\n",
    "    all_macro_POs = {PO.name : PO for PO in parse_domain_file(ftp.FOON_domain_file)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMPL Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OMPL_algorithm = 'RRTConnect'\n",
    "# OMPL_algorithm = 'KPIECE1'\n",
    "# OMPL_algorithm = 'BKPIECE1'\n",
    "OMPL_attempts = 5\n",
    "OMPL_max_compute = 15\n",
    "OMPL_max_simplify = -1\n",
    "OMPL_len_path = 0\n",
    "OMPL_state_resolution = float(\"3.0e-3\")\n",
    "OMPL_use_lua = True\n",
    "OMPL_motion_constraint = \"free\"\n",
    "\n",
    "render_modes = [\"opengl\", \"opengl3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLP (OMPL+skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['OLP', 'FOON']) & set(evaluated_methods)):\n",
    "\n",
    "    method = 'OLP'\n",
    "\n",
    "    setup_time = time.time() - total_time\n",
    "\n",
    "    plan_time = 0\n",
    "\n",
    "    sim_interfacer = Interfacer(\n",
    "        scene_file_name=fpath,\n",
    "        robot_name=coppelia_robot,\n",
    "        robot_gripper=coppelia_gripper,\n",
    "        port_number=coppelia_port_number,\n",
    "    )\n",
    "    sim_interfacer.sim_start()\n",
    "\n",
    "    for i in render_modes:\n",
    "        initial_state_img = os.path.join(os.getcwd(), f'results_{timestamp}/', f'{Path(task_file_name).stem}-{i}.png')\n",
    "        sim_interfacer.sim_take_snapshot(file_name=initial_state_img, render_mode=i)\n",
    "\n",
    "    # NOTE: counters for macro- and micro-level steps:\n",
    "    macro_count, micro_count = 0, 0\n",
    "\n",
    "    macro_plan = []\n",
    "\n",
    "    total_success = 0\n",
    "\n",
    "    all_micro_actions = []\n",
    "\n",
    "    olp_interaction = list(interaction)\n",
    "\n",
    "    print(\"OLP/FOON:\")\n",
    "    print('-- User Task:', user_task['query'])\n",
    "\n",
    "    # -- before executing the plan, we will prompt the LLM for grounding all objects in the FOON to simulation objects:\n",
    "    objects_in_FOON = []\n",
    "    for N in range(len(fga.FOON_lvl3)):\n",
    "        for O in fga.FOON_lvl3[N].getInputNodes() + ftp.fga.FOON_lvl3[N].getOutputNodes():\n",
    "            objects_in_FOON.append(ftp._reviseObjectLabels(O.getObjectLabel()))\n",
    "            for x in range(O.getNumberOfStates()):\n",
    "                related_obj = O.getRelatedObject(x)\n",
    "                if related_obj:\n",
    "                    objects_in_FOON.append(ftp._reviseObjectLabels(related_obj))\n",
    "\n",
    "    object_mapping, grounding_interaction = llm_grounding_sim_objects(\n",
    "        openai_driver,\n",
    "        objects_in_OLP=objects_in_FOON,\n",
    "        objects_in_sim=sim_interfacer.objects_in_sim,\n",
    "        state_as_text=sim_interfacer.perform_sensing(method=3, check_collision=False),\n",
    "        task=user_task['query'],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    olp_interaction.extend(grounding_interaction)\n",
    "\n",
    "    print(\" -- plan-to-simulation object grounding:\", object_mapping)\n",
    "\n",
    "    for N in range(len(fga.FOON_lvl3)):\n",
    "        # NOTE: this is where we have identified a macro plan's step; here, we check the contents of its PO definition for:\n",
    "        #\t1. preconditions - this will become a sub-problem file's initial states (as predicates)\n",
    "        #\t2. effects - this will become a sub-problem file's goal states (as predicates)\n",
    "\n",
    "        before_planning = time.time()\n",
    "\n",
    "        macro_PO_name = f'{ftp._reviseObjectLabels(ftp.fga.FOON_lvl3[N].getMotion().getMotionLabel())}_{N}'\n",
    "\n",
    "        print(\" -- [FOON-TAMP] : Searching for micro-level plan for '\" + macro_PO_name + \"' macro-PO...\")\n",
    "\n",
    "        # -- try to find this step's matching planning operator definition:\n",
    "        matching_PO_obj = all_macro_POs[macro_PO_name] if macro_PO_name in all_macro_POs else None\n",
    "\n",
    "        # -- when we find the equivalent planning operator, then we proceed to treat it as its own problem:\n",
    "        if not matching_PO_obj:\n",
    "            continue\n",
    "\n",
    "        # -- parse the goal predicates and replace the generic object names with those of the sim objects:\n",
    "        print('-- performing object grounding...')\n",
    "        original_predicates = {\n",
    "            'preconditions': matching_PO_obj.getPreconditions(),\n",
    "            'effects': matching_PO_obj.getEffects()\n",
    "        }\n",
    "        grounded_predicates = dict(original_predicates)\n",
    "\n",
    "        for key in grounded_predicates:\n",
    "            for x in range(len(grounded_predicates[key])):\n",
    "                grounded_pred_parts = []\n",
    "                for obj in grounded_predicates[key][x][1:-1].split(' '):\n",
    "                    # -- some predicate args will be split with trailing parentheses (in the case of \"not\" predicates):\n",
    "                    obj_no_parentheses = obj.replace('(', '').replace(')', '')\n",
    "                    # -- we should only do label swapping if the whole argument exists in the grounding map:\n",
    "                    grounded_pred_parts.append(obj if obj_no_parentheses not in object_mapping else object_mapping[obj_no_parentheses])\n",
    "\n",
    "                # -- overwrite the ungrounded predicate with the grounded in simulation version:\n",
    "                grounded_predicates[key][x] = f\"({' '.join(grounded_pred_parts).strip()})\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\"before grounding:\", json.dumps(original_predicates, indent=4))\n",
    "            print(\"after grounding:\", json.dumps(grounded_predicates, indent=4))\n",
    "\n",
    "        assert bool(grounded_predicates['effects']), f\"Error: empty list of goals! Check the generated micro-problem file '{micro_problem_file}'\"\n",
    "\n",
    "        # -- create sub-problem file (micro-level/task-level):\n",
    "        micro_problem_file = create_problem_file(\n",
    "            micro_fpath=micro_problems_path,\n",
    "            action_name=macro_PO_name,\n",
    "            preconditions=grounded_predicates['preconditions'],\n",
    "            effects=grounded_predicates['effects'],\n",
    "            state=sim_interfacer.perform_sensing(check_collision=False, verbose=False)\n",
    "        )\n",
    "\n",
    "        # -- create step-relevant domain file (micro-level/task-level):\n",
    "        micro_domain_file = create_domain_file(\n",
    "            micro_fpath=micro_problems_path,\n",
    "            template_domain_fpath=robot_domain_file,\n",
    "            objects_in_sim=sim_interfacer.objects_in_sim,\n",
    "            # typing=llm_grounding_pddl_types(openai_driver, sim_interfacer.objects_in_sim),\n",
    "        )\n",
    "\n",
    "        micro_plan_file = None\n",
    "        if not micro_plan_file:\n",
    "            micro_plan_file = 'sas_plan'\n",
    "\n",
    "        print('\\n\\t' + 'step ' + str(N+1) +' -- (' + macro_PO_name + ')')\n",
    "        macro_plan.append('; step ' + str(N+1) + ' -- (' + macro_PO_name + '):')\n",
    "\n",
    "        setup_time += time.time() - before_planning\n",
    "\n",
    "        before_planning = time.time()\n",
    "\n",
    "        # -- try to find a sub-problem plan / solution:\n",
    "        result, fd_time = solve(\n",
    "            find_plan(\n",
    "                domain_file=micro_domain_file,\n",
    "                problem_file=micro_problem_file,\n",
    "                verbose=verbose,\n",
    "            ),\n",
    "            verbose=verbose)\n",
    "\n",
    "        # -- if FD returns something valid, then use FD's time:\n",
    "        if fd_time:\n",
    "            plan_time += fd_time\n",
    "        else:\n",
    "            plan_time += (time.time() - before_planning)\n",
    "\n",
    "        successful_execution = True\n",
    "\n",
    "        if result:\n",
    "            macro_count += 1\n",
    "\n",
    "            # -- open the micro problem file, read each line referring to a micro PO, and save to list:\n",
    "            print('\\t-- micro-level plan found as follows:')\n",
    "            micro_plan = []\n",
    "\n",
    "            if os.path.exists(os.path.join(os.getcwd(), micro_plan_file)):\n",
    "                with open(micro_plan_file, 'r') as micro_file:\n",
    "                    for L in micro_file:\n",
    "                        if L.startswith('('):\n",
    "                            # -- parse the line and remove trailing newline character:\n",
    "                            micro_step = L.strip()\n",
    "                            micro_plan.append(micro_step)\n",
    "\n",
    "                            # -- print entire plan to the command line in format of X.Y,\n",
    "                            #       where X is the macro-step count and Y is the micro-step count:\n",
    "                            print('\\t\\t' + str(N+1) + '.' + str(len(micro_plan)) + '\\t:\\t' + micro_step)\n",
    "\n",
    "            micro_count += len(micro_plan)\n",
    "\n",
    "            all_micro_actions.extend(micro_plan)\n",
    "\n",
    "            print(f\"\\n{'*' * 10} ROBOT EXECUTION {'*' * 10}\")\n",
    "\n",
    "            for x in range(len(micro_plan)):\n",
    "                micro_step = micro_plan[x]\n",
    "                print(f\"-- running step {N+1}.{x+1}: {micro_step} -- \", end=\"\")\n",
    "\n",
    "                if 'pick' in micro_step:\n",
    "                    target_object = micro_step[1:-1].split(' ')[1]\n",
    "                elif 'place' in micro_step:\n",
    "                    target_object = micro_step[1:-1].split(' ')[2]\n",
    "\n",
    "                # NOTE: seems like Fast-Downward makes everything lowercase...\n",
    "                for obj in sim_interfacer.objects_in_sim:\n",
    "                    if obj.lower() == target_object.lower():\n",
    "                        target_object = obj\n",
    "\n",
    "                print(target_object, f\"pick={bool('pick' in micro_step)}\", \"...\", end=\"\")\n",
    "\n",
    "                try:\n",
    "                    if 'pick' in micro_step:\n",
    "                        result = sim_interfacer.pick(\n",
    "                            target_object,\n",
    "                            ompl_args={\n",
    "                                \"ompl_algorithm\": OMPL_algorithm,\n",
    "                                \"ompl_num_attempts\": OMPL_attempts,\n",
    "                                \"ompl_max_compute\": OMPL_max_compute,\n",
    "                                \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                                \"ompl_len_path\": OMPL_len_path,\n",
    "                                \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                                \"ompl_use_lua\": OMPL_use_lua,\n",
    "                                \"ompl_motion_constraint\": \"free\",\n",
    "                            },\n",
    "                            affordance='pick-top',\n",
    "                        )\n",
    "                    elif 'place' in micro_step:\n",
    "                        result = sim_interfacer.place(\n",
    "                            target_object,\n",
    "                            ompl_args={\n",
    "                                \"ompl_algorithm\": OMPL_algorithm,\n",
    "                                \"ompl_num_attempts\": OMPL_attempts,\n",
    "                                \"ompl_max_compute\": OMPL_max_compute,\n",
    "                                \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                                \"ompl_len_path\": OMPL_len_path,\n",
    "                                \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                                \"ompl_use_lua\": OMPL_use_lua,\n",
    "                                \"ompl_motion_constraint\": \"free\",\n",
    "                            },\n",
    "                            affordance='place-top',\n",
    "                        )\n",
    "                    else:\n",
    "                        raise Exception(\"Invalid action!\")\n",
    "\n",
    "                    sim_interfacer.sim_pause()\n",
    "\n",
    "                except Exception as e:\n",
    "                    traceback.print_exc()\n",
    "                    sim_interfacer.return_home(method=1)\n",
    "                    successful_execution = False\n",
    "\n",
    "                print(f\" {'success' if result else 'failed'}!\")\n",
    "                total_success += int(result)\n",
    "                if terminate_upon_failure and not result:\n",
    "                    successful_execution = False\n",
    "                    break\n",
    "\n",
    "            if not successful_execution:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            print('\\t-- no micro-level plan found!')\n",
    "            successful_execution = False\n",
    "        print()\n",
    "\n",
    "    print(f\"{'*' * 30}\\n\\n  RESULTS (method=\\\"{method}\\\"):\")\n",
    "    print(f\"\\t-- total number of macro-actions: {macro_count}\")\n",
    "    print(f\"\\t\\t-- success: {macro_count/ len(ftp.fga.FOON_lvl3) * 100.0}% ({macro_count} / {len(ftp.fga.FOON_lvl3)}))\")\n",
    "    print(f\"\\t-- total number of micro-actions: {micro_count}\")\n",
    "    print(f\"\\t\\t-- success: {(total_success / micro_count * 100.0 if micro_count != 0 else 0.0)}% ({total_success} / {micro_count})\")\n",
    "\n",
    "    with open(f'results_{timestamp}/plan_{method}.txt', 'w') as plan_file:\n",
    "        for x in range(len(all_micro_actions)):\n",
    "            plan_file.write(f\"{all_micro_actions[x]}\\n\")\n",
    "\n",
    "    # -- count total number of tokens required by method:\n",
    "    total_tokens = 0\n",
    "    for msg in olp_interaction:\n",
    "        total_tokens += openai_driver.num_tokens(msg['content'])\n",
    "    print(f\"\\t-- total number of tokens: {total_tokens}\")\n",
    "\n",
    "    json.dump(olp_interaction, open(f'results_{timestamp}/interaction_{method}.json', 'w'))\n",
    "\n",
    "    time_taken = f'total time taken: {sim_interfacer.get_elapsed_time()}'\n",
    "    print(f'\\n{time_taken}')\n",
    "    sim_interfacer.sim_print(time_taken)\n",
    "\n",
    "    sim_interfacer.return_home(method=1)\n",
    "    sim_interfacer.sim_pause()\n",
    "\n",
    "    # -- take a screenshot of the final state of the world:\n",
    "    for i in render_modes:\n",
    "        final_state_img = os.path.join(os.getcwd(), f'results_{timestamp}/', f'{Path(task_file_name).stem}-{method}-{i}.png')\n",
    "        sim_interfacer.sim_take_snapshot(file_name=final_state_img, render_mode=i)\n",
    "\n",
    "    experimental_results.append({\n",
    "        'result_id': timestamp,\n",
    "        'method': method,\n",
    "        'num_blocks': len(sim_interfacer.objects_in_sim),\n",
    "        'total_robot_actions': micro_count,\n",
    "        'total_subgoals': len(ftp.fga.FOON_lvl3),\n",
    "        'total_plan_setup_time': setup_time,\n",
    "        'total_plan_solve_time': plan_time,\n",
    "        'total_time': setup_time + plan_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'success': int(successful_execution),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 :- Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-Planner (OMPL+skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['LLM-Planner']) & set(evaluated_methods)):\n",
    "\n",
    "    method = 'LLM-Planner'\n",
    "\n",
    "    total_time = time.time()\n",
    "\n",
    "    # -- use the LLM as a planner to acquire a task plan, where each step will be executed using programmed skills:\n",
    "    sim_interfacer = Interfacer(\n",
    "        scene_file_name=fpath,\n",
    "        robot_name=coppelia_robot,\n",
    "        robot_gripper=coppelia_gripper,\n",
    "        port_number=coppelia_port_number,\n",
    "    )\n",
    "    sim_interfacer.sim_start()\n",
    "\n",
    "    # -- we are going to feed the LLM with the prompt of coming up with a plan using the pre-defined skills:\n",
    "    llm_planner_sys_prompt = open('llm_prompts/llm_planner_system_prompt.txt', 'r').read()\n",
    "\n",
    "    prompt_context = f\"There is a scenario with the following objects: {sim_interfacer.objects_in_sim}. Please await further instructions.\"\n",
    "    print(\"objects presently in scene:\", sim_interfacer.objects_in_sim)\n",
    "\n",
    "    interaction = [\n",
    "        {\"role\": \"system\", \"content\": llm_planner_sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_context}\n",
    "    ]\n",
    "\n",
    "    _, response = openai_driver.prompt(interaction)\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": response}])\n",
    "\n",
    "    prompt_goal = (\n",
    "        f\"Your task is as follows: {user_task['query']}.\"\n",
    "        \" Transform this instruction into a PDDL goal specification in terms of 'on' relations. Do not add any explanation.\"\n",
    "    )\n",
    "\n",
    "    interaction.extend([{\"role\": \"user\", \"content\": prompt_goal}])\n",
    "\n",
    "    _, response = openai_driver.prompt(interaction)\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": response}])\n",
    "\n",
    "    print(f\"LLM-generated goal: {response}\")\n",
    "\n",
    "    llm_planner_user_prompt = (\n",
    "        \"Find a task plan in PDDL to achieve this goal given the initial state below.\"\n",
    "        \" Only specify the list of actions needed.\"\n",
    "        \" Use the actions defined above. Do not add any explanation.\\n\\n\"\n",
    "        f\"Initial state:\\n{sim_interfacer.perform_sensing(method=3).replace('air', 'nothing')}\"\n",
    "    )\n",
    "\n",
    "    interaction.extend([{\"role\": \"user\", \"content\": llm_planner_user_prompt}])\n",
    "\n",
    "    _, response = openai_driver.prompt(interaction)\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": response}])\n",
    "\n",
    "    steps = response.split('\\n')\n",
    "    parsed_steps = []\n",
    "\n",
    "    # -- first, we need to parse through the plan that the LLM gives us by splitting:\n",
    "    total_steps = 0\n",
    "\n",
    "    llm_planner_dir = os.path.join(os.getcwd(), f'results_{timestamp}/', 'output_llm-plan')\n",
    "    if not os.path.exists(llm_planner_dir):\n",
    "        os.mkdir(llm_planner_dir)\n",
    "\n",
    "    # -- write the PDDL domain file generated by the LLM to a file:\n",
    "    llm_planner_files = f\"{llm_planner_dir}/problem-{Path(task_file_name).stem}\"\n",
    "\n",
    "    if not os.path.exists(llm_planner_files):\n",
    "        os.mkdir(llm_planner_files)\n",
    "\n",
    "    while True:\n",
    "        total_steps += 1\n",
    "        found = False\n",
    "\n",
    "        for x in range(len(steps)):\n",
    "            if steps[x].startswith(f\"{total_steps}.\"):\n",
    "                parsed_steps.append(steps[x])\n",
    "                found = True\n",
    "\n",
    "        if not found: break\n",
    "\n",
    "    print(\"Complete list of actions:\")\n",
    "    print(\"\\n\".join(parsed_steps))\n",
    "\n",
    "    total_time = time.time() - total_time\n",
    "    plan_time = total_time\n",
    "\n",
    "    print(f\"\\n{'*' * 25}\\n\")\n",
    "\n",
    "    # -- now that we know the total number of steps, we can go ahead and start to execute each step:\n",
    "    total_success = 0\n",
    "\n",
    "    print('LLM-Planner')\n",
    "    print('-- User Task:', user_task['query'])\n",
    "\n",
    "    successful_execution = True\n",
    "\n",
    "    if parsed_steps:\n",
    "\n",
    "        # -- write the LLM plan's to a file:\n",
    "        with open(f\"{llm_planner_files}/task.plan\", \"w\") as plan_file:\n",
    "            for x in range(len(parsed_steps)):\n",
    "                micro_step = re.search(\"\\(.+\\)\", parsed_steps[x])[0]\n",
    "                plan_file.write(f\"{micro_step}\\n\")\n",
    "\n",
    "        with open(f'results_{timestamp}/plan_{method}.txt', 'w') as plan_file:\n",
    "            for x in range(len(parsed_steps)):\n",
    "                micro_step = re.search(\"\\(.+\\)\", parsed_steps[x])[0]\n",
    "                plan_file.write(f\"{micro_step}\\n\")\n",
    "\n",
    "        for x in range(len(parsed_steps)):\n",
    "            # -- use regex to help us split the string into smaller components:\n",
    "            micro_step = re.search(\"\\(.+\\)\", parsed_steps[x])[0]\n",
    "\n",
    "            print(f\"-- running step {x+1}: {micro_step} -- \", end=\"\")\n",
    "\n",
    "            if 'pick' in micro_step:\n",
    "                target_object = micro_step[1:-1].split(' ')[1]\n",
    "            elif 'place' in micro_step:\n",
    "                target_object = micro_step[1:-1].split(' ')[2]\n",
    "\n",
    "            try:\n",
    "                if 'pick' in micro_step:\n",
    "                    result = sim_interfacer.pick(\n",
    "                        target_object,\n",
    "                        ompl_args={\n",
    "                            \"ompl_algorithm\": OMPL_algorithm,\n",
    "                            \"ompl_num_attempts\": OMPL_attempts,\n",
    "                            \"ompl_max_compute\": OMPL_max_compute,\n",
    "                            \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                            \"ompl_len_path\": OMPL_len_path,\n",
    "                            \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                            \"ompl_use_lua\": OMPL_use_lua,\n",
    "                            \"ompl_motion_constraint\": \"free\",\n",
    "                        },\n",
    "                        affordance='pick-top',\n",
    "                    )\n",
    "                elif 'place' in micro_step:\n",
    "                    result = sim_interfacer.place(\n",
    "                        target_object,\n",
    "                        ompl_args={\n",
    "                            \"ompl_algorithm\": OMPL_algorithm,\n",
    "                            \"ompl_num_attempts\": OMPL_attempts,\n",
    "                            \"ompl_max_compute\": OMPL_max_compute,\n",
    "                            \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                            \"ompl_len_path\": OMPL_len_path,\n",
    "                            \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                            \"ompl_use_lua\": OMPL_use_lua,\n",
    "                            \"ompl_motion_constraint\": \"free\",\n",
    "                        },\n",
    "                        affordance='place-top',\n",
    "                    )\n",
    "                else:\n",
    "                    raise Exception(\"Invalid action!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                sim_interfacer.return_home(method=1)\n",
    "                successful_execution = False\n",
    "                break\n",
    "            else:\n",
    "                print(f\" {'success' if result else 'failed'}!\")\n",
    "                total_success += int(result)\n",
    "                if terminate_upon_failure and not result:\n",
    "                    successful_execution = False\n",
    "                    break\n",
    "\n",
    "        print('\\n\\n% Success rate:', float(total_success / len(parsed_steps) * 100.0), f\"({total_success}/{len(parsed_steps)})\" )\n",
    "\n",
    "    else:\n",
    "        successful_execution = False\n",
    "\n",
    "    # -- count total number of tokens required by method:\n",
    "    total_tokens = 0\n",
    "    for msg in interaction:\n",
    "        total_tokens += openai_driver.num_tokens(msg['content'])\n",
    "    print(f\"\\t-- total number of tokens: {total_tokens}\")\n",
    "\n",
    "    json.dump(interaction, open(f'results_{timestamp}/interaction_{method}.json', 'w'))\n",
    "\n",
    "    time_taken = f'total time taken: {sim_interfacer.get_elapsed_time()}'\n",
    "    print(f'\\n{time_taken}')\n",
    "    sim_interfacer.sim_print(time_taken)\n",
    "\n",
    "    sim_interfacer.return_home(method=1)\n",
    "    sim_interfacer.sim_pause()\n",
    "\n",
    "    # -- take a screenshot of the final state of the world:\n",
    "    for i in render_modes:\n",
    "        final_state_img = os.path.join(os.getcwd(), f'results_{timestamp}/', f'{Path(task_file_name).stem}-{method}-{i}.png')\n",
    "        sim_interfacer.sim_take_snapshot(file_name=final_state_img, render_mode=i)\n",
    "\n",
    "    experimental_results.append({\n",
    "        'result_id': timestamp,\n",
    "        'method': method,\n",
    "        'num_blocks': len(sim_interfacer.objects_in_sim),\n",
    "        'total_robot_actions': len(parsed_steps),\n",
    "        'total_subgoals': '-',\n",
    "        'total_plan_setup_time': plan_time,\n",
    "        'total_plan_solve_time': plan_time,\n",
    "        'total_time': plan_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'success': int(successful_execution),\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~LLM+P (generate problem file, use FD, run OMPL+skills)\n",
    " \n",
    "**NOTE**: This method is akin to [LLM+P (Liu et al. 2023)](https://github.com/Cranial-XIX/llm-pddl/tree/main). \n",
    "\n",
    "We adopt a similar approach where we do the following steps:\n",
    "1. <u>Problem file generation</u> - given an example problem file, the current state of the environment, and the task, the LLM will generate a PDDL problem file that matches the task.\n",
    "2. <u>PDDL planning</u> - use a PDDL solver to find a task plan using a pre-defined domain file.\n",
    "3. <u>Robot execution</u> - if a plan was found in the previous step, run it with OMPL-based skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['LLM+P']) & set(evaluated_methods)):\n",
    "\n",
    "    method = 'LLM+P'\n",
    "\n",
    "    setup_time = time.time()\n",
    "\n",
    "    sim_interfacer = Interfacer(\n",
    "        scene_file_name=fpath,\n",
    "        robot_name=coppelia_robot,\n",
    "        robot_gripper=coppelia_gripper,\n",
    "        port_number=coppelia_port_number,\n",
    "    )\n",
    "    sim_interfacer.sim_start()\n",
    "\n",
    "    llm_plus_p_dir = os.path.join(os.getcwd(), f'results_{timestamp}/', 'output_llm+p')\n",
    "    if not os.path.exists(llm_plus_p_dir):\n",
    "        os.mkdir(llm_plus_p_dir)\n",
    "\n",
    "    pddl_sys_prompt_file = \"llm_prompts/llm+p_system_prompt.txt\"\n",
    "\n",
    "    pddl_system_prompt = open(pddl_sys_prompt_file, 'r').read().replace('<problem_file_example>', top_fewshot_examples(\n",
    "        openai_driver, fewshot_examples, user_task['query'], method=['llm+p', setting],)[0]['pddl'])\n",
    "\n",
    "    interaction = [{\"role\": \"system\", \"content\": pddl_system_prompt}]\n",
    "\n",
    "    pddl_user_prompt = (\"Now I have a new planning problem and its description is as follows:\\n\"\n",
    "                        f\"These objects are on the table: {sim_interfacer.objects_in_sim}.\"\n",
    "                        f\" The current state of the world is:\\n{sim_interfacer.perform_sensing(method=3, check_collision=False)}.\")\n",
    "\n",
    "    interaction.extend([{\"role\": \"user\", \"content\": pddl_user_prompt}])\n",
    "\n",
    "    pddl_user_prompt = (f\"\\nYour goal is to achieve this task: {user_task['query']}. \"\n",
    "                        \"Provide me with the problem PDDL file that describes the new planning problem directly without further explanations.\")\n",
    "\n",
    "    interaction.extend([{\"role\": \"user\", \"content\": pddl_user_prompt}])\n",
    "\n",
    "    _, response = openai_driver.prompt(chat_history=interaction, verbose=False)\n",
    "\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": response}])\n",
    "    if verbose:\n",
    "        print(json.dumps(interaction, indent=4))\n",
    "\n",
    "    llm_plus_p_pddl_files = f\"{llm_plus_p_dir}/problem-{Path(task_file_name).stem}\"\n",
    "    if os.path.exists(llm_plus_p_pddl_files):\n",
    "        shutil.rmtree(llm_plus_p_pddl_files)\n",
    "    os.mkdir(llm_plus_p_pddl_files)\n",
    "\n",
    "    llm_plus_p_problem_file = f\"{llm_plus_p_pddl_files}/problem.pddl\"\n",
    "\n",
    "    with open(llm_plus_p_problem_file, \"w\") as llm_plus_p_problem:\n",
    "        llm_plus_p_problem.write(parse_llm_code(response, separator=\"\\n\"))\n",
    "\n",
    "    # -- create step-relevant domain file (task-level):\n",
    "    llm_plus_p_domain_file = create_domain_file(\n",
    "        micro_fpath=llm_plus_p_pddl_files,\n",
    "        template_domain_fpath=robot_domain_file,\n",
    "        objects_in_sim=sim_interfacer.objects_in_sim,\n",
    "        domain_name=setting,\n",
    "        # typing=llm_grounding_pddl_types(openai_driver, sim_interfacer.objects_in_sim),\n",
    "    )\n",
    "\n",
    "    micro_plan = []\n",
    "\n",
    "    setup_time = time.time() - setup_time\n",
    "\n",
    "    before_planning = time.time()\n",
    "\n",
    "    # -- try to find a sub-problem plan / solution:\n",
    "    result, fd_time = solve(\n",
    "        find_plan(\n",
    "            domain_file=llm_plus_p_domain_file,\n",
    "            problem_file=llm_plus_p_problem_file,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        verbose=verbose)\n",
    "\n",
    "    # -- if FD returns something valid, then use FD's time:\n",
    "    plan_time = (time.time() - before_planning)\n",
    "    if fd_time:\n",
    "        plan_time = fd_time\n",
    "\n",
    "    successful_execution = True\n",
    "\n",
    "    if result:\n",
    "        print(f\"\\n{'*' * 25}\\n\")\n",
    "\n",
    "        print('LLM+P plan:')\n",
    "        print('-- User Task:', user_task['query'])\n",
    "\n",
    "        print('\\t-- micro-level plan found as follows:')\n",
    "\n",
    "        # -- open the micro problem file, read each line referring to a micro PO, and save to list:\n",
    "        if os.path.exists(\"sas_plan\"):\n",
    "            with open('sas_plan', 'r') as micro_file:\n",
    "                for L in micro_file:\n",
    "                    if L.startswith('('):\n",
    "                        # -- parse the line and remove trailing newline character:\n",
    "                        micro_step = L.strip()\n",
    "                        micro_plan.append(micro_step)\n",
    "\n",
    "                        # -- print entire plan to the command line in format of X.Y,\n",
    "                        #       where X is the macro-step count and Y is the micro-step count:\n",
    "                        print('\\t\\t' + str(macro_count) + '.' + str(len(micro_plan)) + '\\t:\\t' + micro_step)\n",
    "\n",
    "            os.remove(\"sas_plan\")\n",
    "\n",
    "            with open(f'results_{timestamp}/plan_{method}.txt', 'w') as plan_file:\n",
    "                for x in range(len(micro_plan)):\n",
    "                    plan_file.write(f\"{micro_plan[x]}\\n\")\n",
    "\n",
    "\n",
    "        print(f\"\\n{'*' * 25}\\n\")\n",
    "\n",
    "        print('LLM+P execution:')\n",
    "\n",
    "        total_success = 0\n",
    "\n",
    "        for x in range(len(micro_plan)):\n",
    "            micro_step = micro_plan[x]\n",
    "            print(f\"-- running step {macro_count}.{x+1}: {micro_step}... \", end=\"\")\n",
    "\n",
    "            if 'pick' in micro_step:\n",
    "                target_object = micro_step[1:-1].split(' ')[1]\n",
    "            elif 'place' in micro_step:\n",
    "                target_object = micro_step[1:-1].split(' ')[2]\n",
    "\n",
    "            # NOTE: seems like Fast-Downward makes everything lowercase...\n",
    "            for obj in sim_interfacer.objects_in_sim:\n",
    "                if obj.lower() == target_object.lower():\n",
    "                    target_object = obj\n",
    "\n",
    "            print(target_object, f\"pick={bool('pick' in micro_step)}\", '...', end='')\n",
    "\n",
    "            try:\n",
    "                if 'pick' in micro_step:\n",
    "                    result = sim_interfacer.pick(\n",
    "                        target_object,\n",
    "                        ompl_args={\n",
    "                            \"ompl_algorithm\": OMPL_algorithm,\n",
    "                            \"ompl_num_attempts\": OMPL_attempts,\n",
    "                            \"ompl_max_compute\": OMPL_max_compute,\n",
    "                            \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                            \"ompl_len_path\": OMPL_len_path,\n",
    "                            \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                            \"ompl_use_lua\": OMPL_use_lua,\n",
    "                            \"ompl_motion_constraint\": \"free\",\n",
    "                        },\n",
    "                        affordance='pick-top',\n",
    "                    )\n",
    "                elif 'place' in micro_step:\n",
    "                    result = sim_interfacer.place(\n",
    "                        target_object,\n",
    "                        ompl_args={\n",
    "                            \"ompl_algorithm\": OMPL_algorithm,\n",
    "                            \"ompl_num_attempts\": OMPL_attempts,\n",
    "                            \"ompl_max_compute\": OMPL_max_compute,\n",
    "                            \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                            \"ompl_len_path\": OMPL_len_path,\n",
    "                            \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                            \"ompl_use_lua\": OMPL_use_lua,\n",
    "                            \"ompl_motion_constraint\": \"free\",\n",
    "                        },\n",
    "                        affordance='place-top',\n",
    "                    )\n",
    "                else:\n",
    "                    raise Exception(\"Invalid action!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                sim_interfacer.return_home()\n",
    "                successful_execution = False\n",
    "                break\n",
    "            else:\n",
    "                print(f\" {'success' if result else 'failed'}!\")\n",
    "                total_success += int(result)\n",
    "                if terminate_upon_failure and not result:\n",
    "                    successful_execution = False\n",
    "                    break\n",
    "\n",
    "        print(f\"{'*' * 30}\\n\\n  RESULTS (method=\\\"{method}\\\"):\")\n",
    "        print(f\"\\t-- total number of micro-actions: {len(micro_plan)}\")\n",
    "        if len(micro_plan):\n",
    "            print(f\"\\t\\t-- success: {total_success / len(micro_plan) * 100.0}%\")\n",
    "        else:\n",
    "            print(\"\\t\\t-- success: 100.0%\")\n",
    "    else:\n",
    "        print(f\"{'*' * 30}\\n\\n  RESULTS (method=\\\"{method}\\\"):\")\n",
    "        print(\"\\t-- no plan found!\\n\\t\\tsuccess: 0%\")\n",
    "        successful_execution = False\n",
    "\n",
    "    # -- count total number of tokens required by method:\n",
    "    total_tokens = 0\n",
    "    for msg in interaction:\n",
    "        total_tokens += openai_driver.num_tokens(msg['content'])\n",
    "    print(f\"\\t-- total number of tokens: {total_tokens}\")\n",
    "\n",
    "    json.dump(interaction, open(f'results_{timestamp}/interaction_{method}.json', 'w'))\n",
    "\n",
    "    experimental_results.append({\n",
    "        'result_id': timestamp,\n",
    "        'method': method,\n",
    "        'num_blocks': len(sim_interfacer.objects_in_sim),\n",
    "        'total_robot_actions': len(micro_plan),\n",
    "        'total_subgoals': '-',\n",
    "        'total_plan_setup_time': setup_time,\n",
    "        'total_plan_solve_time': plan_time,\n",
    "        'total_time': setup_time + plan_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'success': int(successful_execution),\n",
    "    })\n",
    "\n",
    "    time_taken = f'total time taken: {sim_interfacer.get_elapsed_time()}'\n",
    "    print(f'\\n{time_taken}')\n",
    "    sim_interfacer.sim_print(time_taken)\n",
    "\n",
    "    sim_interfacer.return_home(method=1)\n",
    "    sim_interfacer.sim_pause()\n",
    "\n",
    "    # -- take a screenshot of the final state of the world:\n",
    "    for i in render_modes:\n",
    "        final_state_img = os.path.join(os.getcwd(), f'results_{timestamp}/', f'{Path(task_file_name).stem}-{method}-{i}.png')\n",
    "        sim_interfacer.sim_take_snapshot(file_name=final_state_img, render_mode=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~DELTA (generate domain+problem files, run FD, run OMPL+skills)\n",
    "**NOTE**: This baseline is akin to [DELTA (Liu et al. 2024)](https://arxiv.org/abs/2404.03275) but with slight modifications to work with a similar pipeline. \n",
    "\n",
    "This method works as follows:\n",
    "1. <u>Domain file generation</u> - given an example domain file with pick and place planning operators, the LLM generates a new domain file.\n",
    "2. <u>Problem file generation</u> - given an example problem file, the LLM generates a comprehensive problem file (similar to LLM+P).\n",
    "3. <u>Subgoal problem generation</u> - given an example of subgoal decomposition, the LLM generates a set of subgoals, from which PDDL problem files are generated per subgoal.\n",
    "4. <u>PDDL planning</u> - use a PDDL solver to find a task plan using the generated domain file (step 1) and each subgoal problem.\n",
    "5. <u>Robot execution</u> - if a plan was found in the previous step, run it with OMPL-based skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool(set(['DELTA']) & set(evaluated_methods)):\n",
    "\n",
    "    method = 'DELTA'\n",
    "\n",
    "    setup_time = time.time()\n",
    "\n",
    "    sim_interfacer = Interfacer(\n",
    "        scene_file_name=fpath,\n",
    "        robot_name=coppelia_robot,\n",
    "        robot_gripper=coppelia_gripper,\n",
    "        port_number=coppelia_port_number,\n",
    "    )\n",
    "    sim_interfacer.sim_start()\n",
    "\n",
    "    delta_dir = os.path.join(os.getcwd(), f'results_{timestamp}/', 'output_delta')\n",
    "    if not os.path.exists(delta_dir):\n",
    "        os.mkdir(delta_dir)\n",
    "\n",
    "    # -- make a folder for all PDDL files for this task:\n",
    "    delta_pddl_files = f\"{delta_dir}/problem-{Path(task_file_name).stem}\"\n",
    "    if os.path.exists(delta_pddl_files):\n",
    "        shutil.rmtree(delta_pddl_files)\n",
    "    os.mkdir(delta_pddl_files)\n",
    "\n",
    "    best_example = top_fewshot_examples(\n",
    "            openai_driver,\n",
    "            fewshot_examples,\n",
    "            user_task['query'],\n",
    "            method=['delta', setting],)[0]\n",
    "\n",
    "    domain_prompt = open(\"llm_prompts/delta_domain_prompt.txt\", 'r').read().replace(\n",
    "        \"<domain_file_example>\",\n",
    "        best_example['domain_file_prompt']\n",
    "        ).replace(\n",
    "            \"<objects_in_sim>\",\n",
    "            str(sim_interfacer.objects_in_sim)\n",
    "        )\n",
    "\n",
    "    interaction = [{\"role\": \"user\", \"content\": domain_prompt}]\n",
    "    _, pddl_domain = openai_driver.prompt(interaction, verbose=False)\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": pddl_domain}])\n",
    "\n",
    "    if verbose:\n",
    "        print(pddl_domain)\n",
    "\n",
    "    # -- write the PDDL domain file generated by the LLM to a file:\n",
    "    with open(f\"{delta_pddl_files}/domain.pddl\", \"w\") as domain_file:\n",
    "        domain_file.write(parse_llm_code(pddl_domain, separator=\"\\n\"))\n",
    "\n",
    "    problem_prompt = open(\"llm_prompts/delta_problem_prompt.txt\", 'r').read().replace(\n",
    "        \"<problem_file_example>\",\n",
    "        best_example['problem_file_prompt']\n",
    "        ).replace(\n",
    "            \"<new_task>\",\n",
    "            f\"{user_task['query']}\"\n",
    "        ).replace(\n",
    "            \"<current_state>\",\n",
    "            f\"{sim_interfacer.perform_sensing(method=3, check_collision=False)}\"\n",
    "        )\n",
    "\n",
    "    interaction.extend([{\"role\": \"user\", \"content\": problem_prompt}])\n",
    "    _, pddl_problem = openai_driver.prompt(interaction, verbose=False)\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": pddl_problem}])\n",
    "\n",
    "    if verbose:\n",
    "        print(pddl_problem)\n",
    "\n",
    "    # -- write the PDDL problem file generated by the LLM to a file:\n",
    "    with open(f\"{delta_pddl_files}/problem.pddl\", \"w\") as problem_file:\n",
    "        problem_file.write(parse_llm_code(pddl_problem, separator=\"\\n\"))\n",
    "\n",
    "    # -- now we do subgoal generation:\n",
    "    subtasks_prompt = open(\"llm_prompts/delta_subgoals_prompt.txt\", 'r').read().replace(\n",
    "        \"<subgoals_example>\",\n",
    "        best_example['subgoals_prompt']\n",
    "    )\n",
    "\n",
    "    interaction.extend([{\"role\": \"user\", \"content\": subtasks_prompt}])\n",
    "    _, pddl_subgoals = openai_driver.prompt(interaction, verbose=False)\n",
    "    interaction.extend([{\"role\": \"assistant\", \"content\": pddl_subgoals}])\n",
    "\n",
    "    parsed_subgoals = {}\n",
    "\n",
    "    steps = [x.strip() for x in pddl_subgoals.split('\\n')]\n",
    "\n",
    "    # -- first, we need to parse through the plan that the LLM gives us by splitting:\n",
    "    total_subgoals = 1\n",
    "    total_actions = 0\n",
    "\n",
    "    while True:\n",
    "        found_subgoal = False\n",
    "\n",
    "        for x in range(len(steps)):\n",
    "            if steps[x].startswith(f\"{total_subgoals}. \"):\n",
    "                found_subgoal = True    # -- we found a valid subgoal action specified in natural language\n",
    "                found_pddl = False      # -- we now need to find all related PDDL subgoals\n",
    "\n",
    "                # -- we will compile all subgoals in this dictionary:\n",
    "                parsed_subgoals[total_subgoals] = {'description': steps[x].strip(), 'pddl': []}\n",
    "                count = 0\n",
    "\n",
    "                while True:\n",
    "                    count += 1\n",
    "                    found_pddl = False\n",
    "                    for y in range(x, len(steps)):\n",
    "                        if steps[y].startswith(f\"{total_subgoals}.{count}.\"):\n",
    "                            # -- use regex to extract the PDDL subgoal from text:\n",
    "                            pddl_subgoal = re.search(\"\\(.+\\)\", steps[y])\n",
    "                            if pddl_subgoal:\n",
    "                                parsed_subgoals[total_subgoals]['pddl'].append(pddl_subgoal[0])\n",
    "                                found_pddl = True\n",
    "\n",
    "                    if not found_pddl: break\n",
    "\n",
    "        if not found_subgoal:\n",
    "            total_subgoals -= 1\n",
    "            break\n",
    "\n",
    "        total_subgoals += 1\n",
    "\n",
    "\n",
    "    total_success = 0\n",
    "\n",
    "    all_actions = []\n",
    "\n",
    "    setup_time, plan_time = time.time() - setup_time, 0\n",
    "\n",
    "    successful_execution = True\n",
    "\n",
    "    print('DELTA')\n",
    "    print('-- User Task:', user_task['query'])\n",
    "\n",
    "    if not parsed_subgoals or not total_subgoals:\n",
    "        successful_execution = False\n",
    "    else:\n",
    "        for x in sorted(list(parsed_subgoals.keys())):\n",
    "\n",
    "            more_setup = time.time()\n",
    "\n",
    "            # -- create sub-problem file (micro-level/task-level):\n",
    "            micro_problem_file = create_problem_file(\n",
    "                micro_fpath=delta_pddl_files,\n",
    "                preconditions=[],\n",
    "                effects=parsed_subgoals[x]['pddl'],\n",
    "                action_name=f\"sub_problem-{x}\",\n",
    "                domain_name=re.search(\"\\(:domain.+\\)\", pddl_problem)[0][1:-1].split(' ')[1],\n",
    "                state=sim_interfacer.perform_sensing(check_collision=False, verbose=False),\n",
    "            )\n",
    "\n",
    "            setup_time += time.time() - more_setup\n",
    "\n",
    "            before_planning = time.time()\n",
    "\n",
    "            # -- try to find a sub-problem plan / solution:\n",
    "            result, fd_time = solve(\n",
    "                find_plan(\n",
    "                    domain_file=f\"{delta_pddl_files}/domain.pddl\",\n",
    "                    problem_file=f\"{delta_pddl_files}/sub_problem-{x}.pddl\",\n",
    "                    verbose=verbose,\n",
    "                ),\n",
    "                verbose=verbose)\n",
    "\n",
    "            # -- if FD returns something valid, then use FD's time:\n",
    "            if fd_time:\n",
    "                plan_time += fd_time\n",
    "            else:\n",
    "                plan_time += time.time() - before_planning\n",
    "\n",
    "            if result:\n",
    "                print(f\"\\n{'*' * 25}\\n\")\n",
    "\n",
    "                print(f\"\\t-- plan found for sub-goal problem {x}: \\\"{parsed_subgoals[x]['description']}\\\"!\")\n",
    "\n",
    "                # -- open the micro problem file, read each line referring to a micro PO, and save to list:\n",
    "                micro_plan = []\n",
    "\n",
    "                micro_plan_file = 'sas_plan'\n",
    "                if os.path.exists(micro_plan_file):\n",
    "                    with open(micro_plan_file, 'r') as micro_file:\n",
    "                        for L in micro_file:\n",
    "                            if L.startswith('('):\n",
    "                                # -- parse the line and remove trailing newline character:\n",
    "                                micro_step = L.strip()\n",
    "                                micro_plan.append(micro_step)\n",
    "\n",
    "                                # -- print entire plan to the command line in format of X.Y,\n",
    "                                #       where X is the macro-step count and Y is the micro-step count:\n",
    "                                print('\\t\\t' + str(x) + '.' + str(len(micro_plan)) + '\\t:\\t' + micro_step)\n",
    "\n",
    "                all_actions.extend(micro_plan)\n",
    "\n",
    "                print(f\"\\n{'*' * 25}\\n\")\n",
    "\n",
    "                print('DELTA execution:')\n",
    "\n",
    "                total_success += 1\n",
    "\n",
    "                for y in range(len(micro_plan)):\n",
    "                    micro_step = micro_plan[y]\n",
    "                    print(f\"-- running step {x}.{y+1}: {micro_step}... \", end=\"\")\n",
    "\n",
    "                    if 'pick' in micro_step:\n",
    "                        target_object = micro_step[1:-1].split(' ')[1]\n",
    "                    elif 'place' in micro_step:\n",
    "                        target_object = micro_step[1:-1].split(' ')[2]\n",
    "\n",
    "                    # NOTE: seems like Fast-Downward makes everything lowercase...\n",
    "                    for obj in sim_interfacer.objects_in_sim:\n",
    "                        if obj.lower() == target_object.lower():\n",
    "                            target_object = obj\n",
    "\n",
    "                    print(target_object, f\"pick={bool('pick' in micro_step)}\", '...', end='')\n",
    "\n",
    "                    try:\n",
    "                        if 'pick' in micro_step:\n",
    "                            result = sim_interfacer.pick(\n",
    "                                target_object,\n",
    "                                ompl_args={\n",
    "                                    \"ompl_algorithm\": OMPL_algorithm,\n",
    "                                    \"ompl_num_attempts\": OMPL_attempts,\n",
    "                                    \"ompl_max_compute\": OMPL_max_compute,\n",
    "                                    \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                                    \"ompl_len_path\": OMPL_len_path,\n",
    "                                    \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                                    \"ompl_use_lua\": OMPL_use_lua,\n",
    "                                    \"ompl_motion_constraint\": \"free\",\n",
    "                                },\n",
    "                                affordance='pick-top',\n",
    "                            )\n",
    "                        elif 'place' in micro_step:\n",
    "                            result = sim_interfacer.place(\n",
    "                                target_object,\n",
    "                                ompl_args={\n",
    "                                    \"ompl_algorithm\": OMPL_algorithm,\n",
    "                                    \"ompl_num_attempts\": OMPL_attempts,\n",
    "                                    \"ompl_max_compute\": OMPL_max_compute,\n",
    "                                    \"ompl_max_simplify\": OMPL_max_simplify,\n",
    "                                    \"ompl_len_path\": OMPL_len_path,\n",
    "                                    \"ompl_state_resolution\": OMPL_state_resolution,\n",
    "                                    \"ompl_use_lua\": OMPL_use_lua,\n",
    "                                    \"ompl_motion_constraint\": \"free\",\n",
    "                                },\n",
    "                                affordance='place-top',\n",
    "                            )\n",
    "                        else:\n",
    "                            raise Exception(\"Invalid action!\")\n",
    "                    except Exception as e:\n",
    "                        traceback.print_exc()\n",
    "                        sim_interfacer.return_home()\n",
    "                        successful_execution = False\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\" {'success' if result else 'failed'}!\")\n",
    "                        total_actions += int(result)\n",
    "                        if terminate_upon_failure and not result:\n",
    "                            successful_execution = False\n",
    "                            break\n",
    "\n",
    "                if not successful_execution:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print(f\"\\t-- no plan found for sub-goal problem {x}: \\\"{parsed_subgoals[x]['description']}\\\"!\")\n",
    "                successful_execution = False\n",
    "\n",
    "    print(f\"{'*' * 30}\\n\\n  RESULTS (method=\\\"{method}\\\"):\")\n",
    "    if total_subgoals:\n",
    "        print(f\"\\t-- total number of sub-tasks: {total_subgoals}\")\n",
    "        print(f\"\\t\\t-- success: {total_success / total_subgoals * 100.0}% ({total_success} / {total_subgoals})\")\n",
    "        print(f\"\\t-- total number of micro-actions: {len(all_actions)}\")\n",
    "        if len(all_actions):\n",
    "            print(f\"\\t\\t-- success: {total_actions / len(all_actions) * 100.0}% ({total_actions} / {len(all_actions)})\")\n",
    "        else:\n",
    "            print(f\"\\t\\t-- success: 100.0% ({total_actions} / {len(all_actions)})\")\n",
    "\n",
    "    with open(f'results_{timestamp}/plan_{method}.txt', 'w') as plan_file:\n",
    "        for x in range(len(all_actions)):\n",
    "            plan_file.write(f\"{all_actions[x]}\\n\")\n",
    "\n",
    "    # -- count total number of tokens required by method:\n",
    "    total_tokens = 0\n",
    "    for msg in interaction:\n",
    "        total_tokens += openai_driver.num_tokens(msg['content'])\n",
    "    print(f\"\\t-- total number of tokens: {total_tokens}\")\n",
    "\n",
    "    json.dump(interaction, open(f'results_{timestamp}/interaction_{method}.json', 'w'))\n",
    "\n",
    "    experimental_results.append({\n",
    "        'result_id': timestamp,\n",
    "        'method': method,\n",
    "        'num_blocks': len(sim_interfacer.objects_in_sim),\n",
    "        'total_robot_actions': len(all_actions),\n",
    "        'total_subgoals': total_subgoals,\n",
    "        'total_plan_setup_time': setup_time,\n",
    "        'total_plan_solve_time': plan_time if plan_time > 0 else '-',\n",
    "        'total_time': setup_time + plan_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'success': int(successful_execution)\n",
    "    })\n",
    "\n",
    "    time_taken = f'total time taken: {sim_interfacer.get_elapsed_time()}'\n",
    "    print(f'\\n{time_taken}')\n",
    "    sim_interfacer.sim_print(time_taken)\n",
    "\n",
    "    sim_interfacer.return_home(method=1)\n",
    "    sim_interfacer.sim_pause()\n",
    "\n",
    "    # -- take a screenshot of the final state of the world:\n",
    "    for i in render_modes:\n",
    "        final_state_img = os.path.join(os.getcwd(), f'results_{timestamp}/', f'{Path(task_file_name).stem}-{method}-{i}.png')\n",
    "        sim_interfacer.sim_take_snapshot(file_name=final_state_img, render_mode=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 :- Write all results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_fpath = f'./results_{timestamp}/results.csv'\n",
    "df = pd.DataFrame(experimental_results)\n",
    "display(df)\n",
    "df.to_csv(results_fpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop CoppeliaSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coppelia_process.terminate()\n",
    "# coppelia_process.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
